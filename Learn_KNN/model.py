
import numpy as np
from collections import Counter



# ============================================================
# ğŸ§® FONCTIONS DE DISTANCE
# ============================================================

def distance_euclidienne(x1: np.ndarray, x2: np.ndarray) -> float:
    """
    Calcule la distance euclidienne entre deux points.
    
    Formule mathÃ©matique :
        d(x1, x2) = âˆš( Î£ (x1_i - x2_i)Â² )
    
    C'est la distance "classique" en ligne droite entre deux points.
    
    Exemple :
        x1 = [1, 2], x2 = [4, 6]
        d = âˆš((4-1)Â² + (6-2)Â²) = âˆš(9 + 16) = âˆš25 = 5.0
    """
    return np.sqrt(np.sum((x1 - x2) ** 2))


def distance_manhattan(x1: np.ndarray, x2: np.ndarray) -> float:
    """
    Calcule la distance de Manhattan (L1) entre deux points.
    
    Formule :
        d(x1, x2) = Î£ |x1_i - x2_i|
    
    Imagine que tu marches dans une ville en grille (comme Manhattan) :
    tu ne peux pas couper en diagonale, tu dois tourner aux coins.
    
    Exemple :
        x1 = [1, 2], x2 = [4, 6]
        d = |4-1| + |6-2| = 3 + 4 = 7
    """
    return np.sum(np.abs(x1 - x2))


def distance_minkowski(x1: np.ndarray, x2: np.ndarray, p: int = 2) -> float:
    """
    Calcule la distance de Minkowski (gÃ©nÃ©ralisation).
    
    Formule :
        d(x1, x2) = (Î£ |x1_i - x2_i|^p)^(1/p)
    
    - p=1 â†’ Manhattan
    - p=2 â†’ Euclidienne
    - p=âˆ â†’ Chebyshev (max des diffÃ©rences)
    """
    return np.power(np.sum(np.abs(x1 - x2) ** p), 1 / p)


def train_test_split_manual_p(X, y, test_size=0.2, random_state=None):
    """
    Diviser les donnÃ©es en ensembles d'entraÃ®nement et de test.
    
    ParamÃ¨tres :
        X : features
        y : labels
        test_size : proportion des donnÃ©es Ã  mettre dans le jeu de test
        random_state : graine pour la reproductibilitÃ©
    """
    if random_state is not None:
        np.random.seed(random_state)
    
    n_samples = len(X)
    indices = np.random.permutation(len(X))
    test_samples = int(n_samples*test_size)

    train_indices = indices[test_samples:]
    test_indices = indices[:test_samples]

    
    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]



# ============================================================
# ğŸ” NORMALISATION DES DONNÃ‰ES
# ============================================================

def z_score_normalize(X):
    """
    Normalisation Z-Score (Standardisation) : moyenne=0, Ã©cart-type=1.
    
    Formule : X_std = (X - Î¼) / Ïƒ
    """
    mean = X.mean(axis=0)
    std = X.std(axis=0)
    std[std == 0] = 1
    return (X - mean) / std



# ============================================================
# ğŸ§  CLASSE KNN â€” L'ALGORITHME COMPLET
# ============================================================

class KN_Classifier:
    """
    K-Nearest Neighbors Classifier â€” implÃ©mentÃ© from scratch.
    
    Comment Ã§a marche (en 3 Ã©tapes) :
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. STOCKER : On mÃ©morise toutes les donnÃ©es d'entraÃ®nement
       (KNN est un "lazy learner" â€” il n'apprend rien pendant fit!)
    
    2. CALCULER : Pour chaque nouveau point, on calcule sa distance
       avec TOUS les points d'entraÃ®nement
    
    3. VOTER : On prend les K points les plus proches et on vote
       pour la classe majoritaire
    
    ParamÃ¨tres :
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    k : int
        Nombre de voisins Ã  considÃ©rer.
        - k petit (ex: 1-3) â†’ sensible au bruit, frontiÃ¨res complexes
        - k grand (ex: 15-20) â†’ plus lisse, mais peut ignorer les dÃ©tails
        - RÃ¨gle : k doit Ãªtre IMPAIR pour Ã©viter les Ã©galitÃ©s
    
    metric : str
        La mesure de distance Ã  utiliser ('euclidean', 'manhattan', 'minkowski')
    """
    
    def __init__(self, k: int = 5, metric: str = 'euclidean'):
        # VÃ©rification : k doit Ãªtre un entier positif
        if k < 1:
            raise ValueError(f"k doit Ãªtre â‰¥ 1, reÃ§u : {k}")
        
        self.k = k
        self.metric = metric
        
        # On choisit la fonction de distance
        self._distance_functions = {
            'euclidean': distance_euclidienne,
            'manhattan': distance_manhattan,
            'minkowski': distance_minkowski
        }
        
        if metric not in self._distance_functions:
            raise ValueError(f"MÃ©trique inconnue : {metric}. "
                           f"Choix possibles : {list(self._distance_functions.keys())}")
        
        self._distance_fn = self._distance_functions[metric]
        
        # Ces attributs seront remplis par fit()
        self.X_train = None
        self.y_train = None
    
    def fit(self, X: np.ndarray, y: np.ndarray):
        """
        "EntraÃ®ner" le modÃ¨le = simplement stocker les donnÃ©es.
        
        C'est ce qui fait de KNN un "lazy learner" (apprenant paresseux).
        Pas de calcul pendant l'entraÃ®nement !
        
        ParamÃ¨tres :
            X : np.ndarray de shape (n_samples, n_features)
                Les donnÃ©es d'entraÃ®nement (features)
            y : np.ndarray de shape (n_samples,)
                Les Ã©tiquettes (classes)
        """
        self.X_train = np.array(X)
        self.y_train = np.array(y)
        
        # VÃ©rification : k ne doit pas dÃ©passer le nombre d'Ã©chantillons
        if self.k > len(self.X_train):
            raise ValueError(
                f"k={self.k} est plus grand que le nombre "
                f"d'Ã©chantillons d'entraÃ®nement ({len(self.X_train)})"
            )
        print("Le model a bien ete entraine !!")
        return self  # Pour permettre le chaÃ®nage : model.fit(X, y).predict(X_test)
    
    def predict_single(self, x: np.ndarray):
        """
        PrÃ©dire la classe d'UN SEUL point.
        
        Ã‰tapes :
        1. Calculer la distance entre x et TOUS les points d'entraÃ®nement
        2. Trier par distance croissante
        3. Prendre les K plus proches
        4. Voter : la classe la plus frÃ©quente gagne
        """
        # Ã‰tape 1 : Calculer toutes les distances
        distances = np.array([
            self._distance_fn(x, x_train) 
            for x_train in self.X_train
        ])
        
        # Ã‰tape 2 : Obtenir les indices des K plus petites distances
        # np.argsort() retourne les indices qui trieraient le tableau
        k_indices = np.argsort(distances)[:self.k]
        
        # Ã‰tape 3 : RÃ©cupÃ©rer les classes de ces K voisins
        k_voisins_labels = [self.y_train[i].item() if hasattr(self.y_train[i],'item') else self.y_train[i] for i in k_indices]
        
        # Ã‰tape 4 : Vote majoritaire
        # Counter({'Iris-setosa': 3, 'Iris-versicolor': 2}) â†’ 'Iris-setosa'
        vote = Counter(k_voisins_labels)
    
        classe_predite = vote.most_common(1)[0][0]

        return classe_predite


    def predict(self, X: np.ndarray):
        """
        PrÃ©dire les classes pour un ensemble de points.
        
        ParamÃ¨tres :
            X : np.ndarray de shape (n_samples, n_features)
        
        Retourne :
            np.ndarray de shape (n_samples,) avec les classes prÃ©dites
        """
        if self.X_train is None:
            raise RuntimeError("Le modÃ¨le n'a pas Ã©tÃ© entraÃ®nÃ© ! Appelle fit() d'abord.")
        X = np.array(X)
        predictions = []
        for x in X :
            predictions.append(self.predict_single(x))

        return np.array(predictions)
            
    def predict_proba(self, X: np.ndarray) -> dict:
        """
        Renvoie les probabilitÃ©s de chaque classe pour chaque point.
        
        La "probabilitÃ©" = proportion des K voisins appartenant Ã  chaque classe.
        Exemple avec k=5 : si 3 voisins sont Setosa et 2 sont Versicolor,
                           â†’ P(Setosa) = 3/5 = 0.6, P(Versicolor) = 2/5 = 0.4
        """
        all_probas = []
        X = np.array(X)
        
        for x in X:
            distances = np.array([
                self._distance_fn(x, x_train)
                for x_train in self.X_train
            ])
            k_indices = np.argsort(distances)[:self.k]
            k_labels = self.y_train[k_indices]
            
            vote = Counter(k_labels)
            probas = {
                label: count / self.k 
                for label, count in vote.items()
            }
            all_probas.append(probas)
        
        return all_probas
    
    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """
        Calculer l'accuracy (prÃ©cision globale) du modÃ¨le.
        
        Accuracy = nombre de prÃ©dictions correctes / nombre total de prÃ©dictions
        """
        predictions = self.predict(X)
        y = np.array(y)
        accuracy = np.sum(predictions == np.array(y)) / len(y)
        return accuracy
    
    def __repr__(self):
        return f"KNN(k={self.k}, metric='{self.metric}')"


# ============================================================
# ğŸ“Š FONCTIONS D'Ã‰VALUATION
# ============================================================

def accuracy_score(y_true, y_pred):
    """
    Calculer l'accuracy (prÃ©cision globale).
    
    Accuracy = (nombre de prÃ©dictions correctes) / (nombre total de prÃ©dictions)
    """
    return np.mean(np.array(y_true) == np.array(y_pred))



# ============================================================
# ğŸ” K-NEAREST NEIGHBORS REGRESSOR (REGRESSION)
# ============================================================


class KN_Regressor:
    """
    K-Nearest Neighbors Regressor â€” implÃ©mentÃ© from scratch.
    
    Fonctionnement :
    1. Pour chaque point de test, trouver les K voisins les plus proches
    2. Calculer la moyenne des valeurs des voisins
    3. Retourner cette moyenne comme prÃ©diction
    """
    
    def __init__(self, k=5, metric='euclidean'):
        """
        Initialiser le rÃ©gresseur KNN.
        
        ParamÃ¨tres :
            k : int, nombre de voisins Ã  considÃ©rer
            metric : str, mÃ©trique de distance ('euclidean' ou 'manhattan')
        """
        self.k = k
        self.metric = metric
        
        # Choisir la fonction de distance
        self._distance_functions = {
            'euclidean': distance_euclidienne,
            'manhattan': distance_manhattan,
            'minkowski': distance_minkowski
        }
        
        if metric not in self._distance_functions:
            raise ValueError(f"MÃ©trique inconnue : {metric}. "
                           f"Choix possibles : {list(self._distance_functions.keys())}")
        
        self._distance_fn = self._distance_functions[metric]
        
        # Ces attributs seront remplis par fit()
        self.X_train = None
        self.y_train = None
    
    def fit(self, X: np.ndarray, y: np.ndarray):
        """
        "EntraÃ®ner" le modÃ¨le = simplement stocker les donnÃ©es.
        
        ParamÃ¨tres :
            X : np.ndarray de shape (n_samples, n_features)
            y : np.ndarray de shape (n_samples,)
        """
        self.X_train = np.array(X)
        self.y_train = np.array(y).ravel()  # Toujours 1D pour Ã©viter les problÃ¨mes de broadcasting
        
        if self.k > len(self.X_train):
            raise ValueError(
                f"k={self.k} est plus grand que le nombre "
                f"d'Ã©chantillons d'entraÃ®nement ({len(self.X_train)})"
            )
        
        return self
    
    def _predict_single(self, x: np.ndarray) -> float:
        """
        PrÃ©dire la valeur pour UN SEUL point.
        
        Ã‰tapes :
        1. Calculer la distance entre x et tous les points d'entraÃ®nement
        2. Trier par distance croissante
        3. Prendre les K plus proches
        4. Calculer la moyenne de leurs valeurs
        """
        x = np.array(x)
        # Ã‰tape 1 : Calculer toutes les distances
        distances = np.array([
            self._distance_fn(x, x_train)
            for x_train in self.X_train
        ])
        
        # Ã‰tape 2 : Obtenir les indices des K plus petites distances
        k_indices = np.argsort(distances)[:self.k]
        
        # Ã‰tape 3 : RÃ©cupÃ©rer les valeurs des K plus proches voisins
        k_voisins_values = self.y_train[k_indices]
        
        # Ã‰tape 4 : Calculer la moyenne (rÃ©gression)
        prediction = np.mean(k_voisins_values)
        
        return prediction
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        PrÃ©dire les valeurs pour un ensemble de points.
        
        ParamÃ¨tres :
            X : np.ndarray de shape (n_samples, n_features)
        
        Retourne :
            np.ndarray de shape (n_samples,) avec les valeurs prÃ©dites
        """
        X = np.array(X)
        if self.X_train is None:
            raise RuntimeError("Le modÃ¨le n'a pas Ã©tÃ© entraÃ®nÃ© ! Appelle fit() d'abord.")
        
        predictions = np.array([self._predict_single(x) for x in X])
        return predictions
    
    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """
        Calculer le score RÂ² (coefficient de dÃ©termination).
        
        RÂ² = 1 - (SS_res / SS_tot)
           = 1 - (somme des carrÃ©s des rÃ©sidus / somme des carrÃ©s totaux)
        
        - 1.0 = prÃ©dictions parfaites
        - 0.0 = aussi bon que de prÃ©dire la moyenne
        - < 0 = pire que de prÃ©dire la moyenne
        """
        X = np.array(X)
        y = np.array(y).ravel()  # Toujours 1D pour Ã©viter les problÃ¨mes de broadcasting
        predictions = self.predict(X)
        
        # Calculer SS_res (somme des carrÃ©s des rÃ©sidus)
        somme_residus = np.sum((y - predictions) ** 2)
        
        # Calculer SS_tot (somme des carrÃ©s totaux)
        somme_totale = np.sum((y - np.mean(y)) ** 2)
        
        # Ã‰viter la division par zÃ©ro
        if somme_totale == 0:
            return 1.0 if somme_residus == 0 else 0.0
        
        r2_score = 1 - (somme_residus / somme_totale)
        return r2_score

    def __repr__(self):
        return f"KNNRegressor(k={self.k}, metric='{self.metric}')"
     
# Fonction train test split manual pour datasets non pandas (Non Dataframes)

def train_test_split_manual(X, y, test_size=0.2, random_state=42):
    """
    Divise les donnÃ©es en ensembles d'entraÃ®nement et de test.
    """
    # Fixer la seed pour la reproductibilitÃ©
    if random_state is not None:
        np.random.seed(random_state)
    
    # Conversion en numpy array pour assurer le bon fonctionnement
    X = np.array(X)
    y = np.array(y)
    
    # 1. CrÃ©er et mÃ©langer les indices
    indices = np.arange(len(X))
    np.random.shuffle(indices)
    
    # 2. Calculer le point de coupure
    test_set_size = int(len(X) * test_size)
    
    # 3. SÃ©parer les indices
    test_indices = indices[:test_set_size]
    train_indices = indices[test_set_size:]
    
    # 4. CrÃ©er les sous-ensembles
    X_train, X_test = X[train_indices], X[test_indices]
    y_train, y_test = y[train_indices], y[test_indices]
    
    return X_train, X_test, y_train, y_test
