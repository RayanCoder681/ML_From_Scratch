import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘        ğŸ“š COURS COMPLET : KNN (K-Nearest Neighbors) FROM SCRATCH            â•‘
â•‘                                                                              â•‘
â•‘        Semaine 1 de la Roadmap ML                                           â•‘
â•‘        Dataset : Iris.csv                                                    â•‘
â•‘        Objectif : Comprendre, coder, visualiser et maÃ®triser KNN            â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ Ce script est un cours interactif. ExÃ©cute-le section par section 
   (dans un notebook Jupyter ou directement en Python).

ğŸ“Œ Plan du cours :
   1. La thÃ©orie de KNN (avec analogie)
   2. Charger et explorer le dataset Iris
   3. ImplÃ©menter et tester KNN from scratch
   4. Visualiser les rÃ©sultats
   5. Trouver le meilleur K
   6. Comparer avec scikit-learn
   7. Bonus : frontiÃ¨res de dÃ©cision

PrÃ©requis : NumPy, Pandas, Matplotlib (tous dÃ©jÃ  acquis âœ…)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Backend non-interactif pour sauvegarder les figures
import os
import sys

# Importer notre KNN from scratch
from knn_from_scratch import (
    KNN,
    train_test_split_manual,
    matrice_de_confusion,
    accuracy_score,
    classification_report_manual,
    print_classification_report,
    min_max_normalize,
    z_score_normalize,
    distance_euclidienne,
    distance_manhattan
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“– SECTION 1 : LA THÃ‰ORIE DE KNN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ğŸ“– SECTION 1 : LA THÃ‰ORIE DE KNN               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ§  L'ANALOGIE :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Imagine que tu arrives dans une nouvelle ville et tu veux savoir 
si un quartier est "riche" ou "modeste".

Ta stratÃ©gie ? Tu regardes les 5 maisons les plus proches :
  â†’ Si 4 sur 5 sont des villas â†’ quartier riche !
  â†’ Si 4 sur 5 sont modestes â†’ quartier modeste !

C'est EXACTEMENT ce que fait KNN. Pas de formule magique,
juste du bon sens : "Dis-moi qui sont tes voisins, je te dirai qui tu es."

ğŸ“ LES MATHÃ‰MATIQUES :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KNN a besoin d'UNE SEULE chose : mesurer la DISTANCE entre deux points.

Distance Euclidienne (la plus courante) :
  d(A, B) = âˆš( (Aâ‚-Bâ‚)Â² + (Aâ‚‚-Bâ‚‚)Â² + ... + (Aâ‚™-Bâ‚™)Â² )

Pour 2 dimensions : c'est le thÃ©orÃ¨me de Pythagore !
  d(A, B) = âˆš( (xâ‚-xâ‚‚)Â² + (yâ‚-yâ‚‚)Â² )

âš™ï¸ HYPERPARAMÃˆTRE K :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
K = combien de voisins on regarde

  K=1  â†’ On regarde le PLUS PROCHE voisin uniquement
         âš ï¸ TrÃ¨s sensible au bruit (1 point aberrant change tout)
         
  K=5  â†’ On regarde les 5 voisins les plus proches
         âœ… Plus robuste, lisse les erreurs individuelles
         
  K=50 â†’ On regarde 50 voisins
         âš ï¸ Trop de voisins â†’ on inclut des points trop Ã©loignÃ©s
         
  ğŸ’¡ RÃ¨gle : K doit Ãªtre IMPAIR (pour Ã©viter les Ã©galitÃ©s au vote)
  ğŸ’¡ RÃ¨gle : K â‰¤ âˆšN (racine carrÃ©e du nombre d'Ã©chantillons)

ğŸ“Š AVANTAGES / INCONVÃ‰NIENTS :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… Simple Ã  comprendre et implÃ©menter
  âœ… Pas de phase d'entraÃ®nement (lazy learner)
  âœ… Fonctionne bien sur des petits datasets
  âœ… Pas d'hypothÃ¨se sur la distribution des donnÃ©es
  
  âŒ LENT en prÃ©diction (compare avec TOUS les points)
  âŒ Sensible aux dimensions Ã©levÃ©es (curse of dimensionality)
  âŒ NÃ©cessite de normaliser les donnÃ©es
  âŒ Stocke TOUTES les donnÃ©es en mÃ©moire
""")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒ¸ SECTION 2 : CHARGER ET EXPLORER LE DATASET IRIS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸŒ¸ SECTION 2 : EXPLORER LE DATASET IRIS              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Charger le dataset
# On remonte de 2 niveaux pour aller dans datasetCsv
dataset_path = os.path.join(os.path.dirname(__file__), '..', '..', 'datasetCsv', 'Iris.csv')
df = pd.read_csv(dataset_path)

print("ğŸ“‹ AperÃ§u du dataset Iris :")
print("â”€" * 50)
print(df.head(10))
print(f"\nğŸ“ Dimensions : {df.shape[0]} Ã©chantillons Ã— {df.shape[1]} colonnes")
print(f"\nğŸ“Š Colonnes : {list(df.columns)}")

print("\nğŸ“ˆ Statistiques descriptives :")
print("â”€" * 50)
print(df.describe().round(2))

print("\nğŸ·ï¸ Distribution des espÃ¨ces :")
print("â”€" * 50)
print(df['Species'].value_counts())
print(f"\nâ†’ Dataset PARFAITEMENT Ã‰QUILIBRÃ‰ : 50 de chaque espÃ¨ce âœ…")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š SECTION 3 : VISUALISATION DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸ“Š SECTION 3 : VISUALISATION DES DONNÃ‰ES             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# CrÃ©er un dossier pour les visualisations
output_dir = os.path.join(os.path.dirname(__file__), 'visualisations')
os.makedirs(output_dir, exist_ok=True)

# Couleurs pour les 3 espÃ¨ces
colors = {'Iris-setosa': '#FF6B6B', 'Iris-versicolor': '#4ECDC4', 'Iris-virginica': '#45B7D1'}
species_list = df['Species'].unique()

# â”€â”€ Figure 1 : Scatter plot Petal Length vs Petal Width â”€â”€
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Graphique 1 : PÃ©tales
for species in species_list:
    mask = df['Species'] == species
    axes[0].scatter(
        df[mask]['PetalLengthCm'], 
        df[mask]['PetalWidthCm'],
        c=colors[species], label=species, 
        alpha=0.8, edgecolors='white', s=80
    )
axes[0].set_xlabel('Longueur du PÃ©tale (cm)', fontsize=12)
axes[0].set_ylabel('Largeur du PÃ©tale (cm)', fontsize=12)
axes[0].set_title('ğŸŒ¸ PÃ©tales â€” Les 3 espÃ¨ces se sÃ©parent bien !', fontsize=13, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Graphique 2 : SÃ©pales
for species in species_list:
    mask = df['Species'] == species
    axes[1].scatter(
        df[mask]['SepalLengthCm'], 
        df[mask]['SepalWidthCm'],
        c=colors[species], label=species, 
        alpha=0.8, edgecolors='white', s=80
    )
axes[1].set_xlabel('Longueur du SÃ©pale (cm)', fontsize=12)
axes[1].set_ylabel('Largeur du SÃ©pale (cm)', fontsize=12)
axes[1].set_title('ğŸŒ¿ SÃ©pales â€” Plus de chevauchement ici', fontsize=13, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.suptitle('Exploration visuelle du Dataset Iris', fontsize=15, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, '01_exploration_iris.png'), dpi=150, bbox_inches='tight')
plt.close()
print("âœ… Figure 1 sauvegardÃ©e : visualisations/01_exploration_iris.png")

# â”€â”€ Figure 2 : Distribution de chaque feature â”€â”€
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
feature_names_fr = ['Longueur SÃ©pale', 'Largeur SÃ©pale', 'Longueur PÃ©tale', 'Largeur PÃ©tale']

for idx, (feature, name_fr) in enumerate(zip(features, feature_names_fr)):
    ax = axes[idx // 2][idx % 2]
    for species in species_list:
        mask = df['Species'] == species
        ax.hist(df[mask][feature], bins=15, alpha=0.6, 
                color=colors[species], label=species, edgecolor='white')
    ax.set_xlabel(f'{name_fr} (cm)', fontsize=11)
    ax.set_ylabel('FrÃ©quence', fontsize=11)
    ax.set_title(f'Distribution : {name_fr}', fontsize=12, fontweight='bold')
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.2)

plt.suptitle('Distribution des Features par EspÃ¨ce', fontsize=15, fontweight='bold')
plt.tight_layout()
plt.savefig(os.path.join(output_dir, '02_distributions.png'), dpi=150, bbox_inches='tight')
plt.close()
print("âœ… Figure 2 sauvegardÃ©e : visualisations/02_distributions.png")

# â”€â”€ Figure 3 : Matrice de corrÃ©lation â”€â”€
fig, ax = plt.subplots(figsize=(8, 6))
corr_matrix = df[features].corr()
im = ax.imshow(corr_matrix, cmap='RdYlBu_r', vmin=-1, vmax=1)
ax.set_xticks(range(len(feature_names_fr)))
ax.set_xticklabels(feature_names_fr, rotation=45, ha='right', fontsize=10)
ax.set_yticks(range(len(feature_names_fr)))
ax.set_yticklabels(feature_names_fr, fontsize=10)

# Ajouter les valeurs dans chaque cellule
for i in range(len(features)):
    for j in range(len(features)):
        val = corr_matrix.iloc[i, j]
        color = 'white' if abs(val) > 0.6 else 'black'
        ax.text(j, i, f'{val:.2f}', ha='center', va='center', 
                fontsize=12, fontweight='bold', color=color)

plt.colorbar(im)
plt.title('Matrice de CorrÃ©lation des Features', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig(os.path.join(output_dir, '03_correlation_matrix.png'), dpi=150, bbox_inches='tight')
plt.close()
print("âœ… Figure 3 sauvegardÃ©e : visualisations/03_correlation_matrix.png")

print("""
ğŸ’¡ OBSERVATIONS CLÃ‰S :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Les PÃ‰TALES sont les features les plus discriminantes
   â†’ Setosa est trÃ¨s sÃ©parÃ©e des deux autres
   â†’ Versicolor et Virginica se chevauchent lÃ©gÃ¨rement

2. Les SÃ‰PALES seuls ne suffisent pas pour distinguer les 3 espÃ¨ces

3. CorrÃ©lation forte entre PetalLength et PetalWidth (0.96)
   â†’ Ces deux features portent une information similaire

4. KNN devrait TRÃˆS bien fonctionner sur ce dataset !
""")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§ª SECTION 4 : PRÃ‰PARATION DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸ§ª SECTION 4 : PRÃ‰PARATION DES DONNÃ‰ES               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Extraire les features et le label
X = df[features].values  # shape: (150, 4)
y = df['Species'].values  # shape: (150,)

print(f"X shape : {X.shape} â†’ 150 observations, 4 features chacune")
print(f"y shape : {y.shape} â†’ 150 Ã©tiquettes")

# Train/Test Split from scratch (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split_manual(X, y, test_size=0.2, random_seed=42)

print(f"\nğŸ“Š Split des donnÃ©es :")
print(f"   EntraÃ®nement : {X_train.shape[0]} Ã©chantillons ({X_train.shape[0]/len(X)*100:.0f}%)")
print(f"   Test         : {X_test.shape[0]} Ã©chantillons ({X_test.shape[0]/len(X)*100:.0f}%)")

# VÃ©rifier la distribution dans train et test
from collections import Counter
print(f"\n   Distribution dans train : {dict(Counter(y_train))}")
print(f"   Distribution dans test  : {dict(Counter(y_test))}")

# Normalisation
print("\nğŸ”„ Normalisation Min-Max :")
print(f"   Avant â€” X[0] = {X_train[0].round(2)}")
X_train_norm = min_max_normalize(X_train)
X_test_norm = min_max_normalize(X_test)
print(f"   AprÃ¨s â€” X[0] = {X_train_norm[0].round(4)}")
print(f"   â†’ Toutes les valeurs sont maintenant entre 0 et 1 âœ…")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ SECTION 5 : ENTRAÃNER ET TESTER KNN FROM SCRATCH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ğŸš€ SECTION 5 : KNN FROM SCRATCH â€” LE GRAND MOMENT !     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# CrÃ©er le modÃ¨le avec K=5
model = KNN(k=5, metric='euclidean')
print(f"ModÃ¨le crÃ©Ã© : {model}")

# EntraÃ®ner (= mÃ©moriser les donnÃ©es)
model.fit(X_train_norm, y_train)
print(f"âœ… ModÃ¨le entraÃ®nÃ© sur {len(X_train_norm)} Ã©chantillons")
print(f"   (Rappel : KNN ne 'calcule' rien pendant fit, il mÃ©morise !)\n")

# PrÃ©dire sur le test set
y_pred = model.predict(X_test_norm)

# Afficher quelques prÃ©dictions
print("ğŸ” Exemples de prÃ©dictions :")
print(f"{'#':>3}  {'PrÃ©dit':<20} {'RÃ©el':<20} {'Correct ?':>10}")
print("â”€" * 60)
for i in range(min(15, len(y_test))):
    correct = "âœ…" if y_pred[i] == y_test[i] else "âŒ"
    print(f"{i+1:>3}  {y_pred[i]:<20} {y_test[i]:<20} {correct:>10}")

# Accuracy
acc = accuracy_score(y_test, y_pred)
print(f"\nğŸ¯ ACCURACY = {acc:.4f} ({acc*100:.1f}%)")
print(f"   â†’ {int(acc * len(y_test))}/{len(y_test)} prÃ©dictions correctes")

# Matrice de confusion
print("\nğŸ“Š MATRICE DE CONFUSION :")
print("â”€" * 50)
cm, classes = matrice_de_confusion(y_test, y_pred)
print(f"{'':>20}", end="")
for c in classes:
    print(f"{c.split('-')[1]:>12}", end="")
print()
for i, c in enumerate(classes):
    print(f"{c.split('-')[1]:>20}", end="")
    for j in range(len(classes)):
        val = cm[i][j]
        marker = " âœ…" if i == j else " âŒ" if val > 0 else "   "
        print(f"{val:>10}{marker}", end="")
    print()

# Rapport de classification
report = classification_report_manual(y_test, y_pred)
print_classification_report(report)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ” SECTION 6 : TROUVER LE MEILLEUR K
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘             ğŸ” SECTION 6 : TROUVER LE MEILLEUR K            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¡ K est un HYPERPARAMÃˆTRE : on doit le choisir NOUS-MÃŠMES.
   La meilleure mÃ©thode ? Tester plusieurs valeurs !
""")

# Tester K de 1 Ã  25
k_values = range(1, 26)
accuracies = []
error_rates = []

print(f"{'K':>3}  {'Accuracy':>10}  {'Erreur':>10}  {'Visualisation'}")
print("â”€" * 55)

for k in k_values:
    model_k = KNN(k=k, metric='euclidean')
    model_k.fit(X_train_norm, y_train)
    acc_k = model_k.score(X_test_norm, y_test)
    accuracies.append(acc_k)
    error_rates.append(1 - acc_k)
    
    bar = "â–ˆ" * int(acc_k * 30) + "â–‘" * (30 - int(acc_k * 30))
    marker = " â­" if acc_k == max(accuracies) else ""
    print(f"{k:>3}  {acc_k:>10.4f}  {1-acc_k:>10.4f}  {bar}{marker}")

best_k = list(k_values)[np.argmax(accuracies)]
best_acc = max(accuracies)
print(f"\nğŸ† MEILLEUR K = {best_k} avec Accuracy = {best_acc:.4f} ({best_acc*100:.1f}%)")

# â”€â”€ Figure 4 : Accuracy vs K â”€â”€
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Accuracy
ax1.plot(list(k_values), accuracies, 'o-', color='#4ECDC4', linewidth=2, markersize=8)
ax1.axvline(x=best_k, color='#FF6B6B', linestyle='--', alpha=0.7, label=f'Meilleur K={best_k}')
ax1.fill_between(list(k_values), accuracies, alpha=0.1, color='#4ECDC4')
ax1.set_xlabel('K (nombre de voisins)', fontsize=12)
ax1.set_ylabel('Accuracy', fontsize=12)
ax1.set_title('Accuracy en fonction de K', fontsize=13, fontweight='bold')
ax1.legend(fontsize=11)
ax1.grid(True, alpha=0.3)
ax1.set_xticks(list(k_values))

# Error Rate
ax2.plot(list(k_values), error_rates, 'o-', color='#FF6B6B', linewidth=2, markersize=8)
ax2.axvline(x=best_k, color='#4ECDC4', linestyle='--', alpha=0.7, label=f'Meilleur K={best_k}')
ax2.fill_between(list(k_values), error_rates, alpha=0.1, color='#FF6B6B')
ax2.set_xlabel('K (nombre de voisins)', fontsize=12)
ax2.set_ylabel("Taux d'erreur", fontsize=12)
ax2.set_title("Taux d'erreur en fonction de K", fontsize=13, fontweight='bold')
ax2.legend(fontsize=11)
ax2.grid(True, alpha=0.3)
ax2.set_xticks(list(k_values))

plt.suptitle('SÃ©lection du Meilleur K â€” Hyperparameter Tuning', fontsize=15, fontweight='bold')
plt.tight_layout()
plt.savefig(os.path.join(output_dir, '04_best_k.png'), dpi=150, bbox_inches='tight')
plt.close()
print(f"\nâœ… Figure 4 sauvegardÃ©e : visualisations/04_best_k.png")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ SECTION 7 : IMPACT DE LA DISTANCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ“ SECTION 7 : COMPARAISON DES DISTANCES           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

for metric_name in ['euclidean', 'manhattan']:
    model_m = KNN(k=best_k, metric=metric_name)
    model_m.fit(X_train_norm, y_train)
    acc_m = model_m.score(X_test_norm, y_test)
    print(f"  Distance {metric_name:<12} â†’ Accuracy = {acc_m:.4f} ({acc_m*100:.1f}%)")

# Exemple concret de distance
print("\nğŸ“ Exemple concret :")
point_a = np.array([5.1, 3.5, 1.4, 0.2])
point_b = np.array([7.0, 3.2, 4.7, 1.4])
print(f"   Point A (Setosa)    = {point_a}")
print(f"   Point B (Versicolor) = {point_b}")
print(f"   Distance Euclidienne = {distance_euclidienne(point_a, point_b):.4f}")
print(f"   Distance Manhattan   = {distance_manhattan(point_a, point_b):.4f}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¬ SECTION 8 : PROBABILITÃ‰S DE PRÃ‰DICTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸ”¬ SECTION 8 : PROBABILITÃ‰S DE PRÃ‰DICTION            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¡ KNN ne donne pas juste une classe â€” il peut aussi donner
   la CONFIANCE de sa prÃ©diction !
   
   Si k=5 et les 5 voisins sont Setosa â†’ confiance = 100%
   Si k=5 et 3 sont Setosa, 2 Versicolor â†’ confiance = 60%
""")

model_best = KNN(k=best_k, metric='euclidean')
model_best.fit(X_train_norm, y_train)
probas = model_best.predict_proba(X_test_norm[:5])

print("ProbabilitÃ©s pour les 5 premiers exemples de test :")
print("â”€" * 70)
for i, proba in enumerate(probas):
    print(f"\n  Ã‰chantillon #{i+1} (RÃ©el : {y_test[i]})")
    for cls, p in sorted(proba.items(), key=lambda x: -x[1]):
        bar = "â–ˆ" * int(p * 20) + "â–‘" * (20 - int(p * 20))
        print(f"    {cls:<20} : {p:.2f} {bar}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†š SECTION 9 : COMPARAISON AVEC SCIKIT-LEARN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       ğŸ†š SECTION 9 : COMPARAISON AVEC SCIKIT-LEARN          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Maintenant que tu as tout compris FROM SCRATCH, 
comparons avec la version "professionnelle" de sklearn.
""")

try:
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.metrics import accuracy_score as sklearn_accuracy
    from sklearn.metrics import classification_report as sklearn_report
    
    # Notre version
    our_model = KNN(k=best_k, metric='euclidean')
    our_model.fit(X_train_norm, y_train)
    our_pred = our_model.predict(X_test_norm)
    our_acc = accuracy_score(y_test, our_pred)
    
    # Version sklearn
    sklearn_model = KNeighborsClassifier(n_neighbors=best_k, metric='euclidean')
    sklearn_model.fit(X_train_norm, y_train)
    sklearn_pred = sklearn_model.predict(X_test_norm)
    sklearn_acc = sklearn_accuracy(y_test, sklearn_pred)
    
    print(f"  ğŸ Notre KNN from scratch  â†’ Accuracy = {our_acc:.4f}")
    print(f"  ğŸ“¦ Sklearn KNeighbors      â†’ Accuracy = {sklearn_acc:.4f}")
    
    # Comparer prÃ©diction par prÃ©diction
    same = np.sum(our_pred == sklearn_pred)
    print(f"\n  ğŸ” PrÃ©dictions identiques : {same}/{len(our_pred)} "
          f"({same/len(our_pred)*100:.1f}%)")
    
    if our_acc == sklearn_acc:
        print("\n  âœ… PARFAIT ! Les deux versions donnent le MÃŠME rÃ©sultat !")
        print("     â†’ Notre implÃ©mentation from scratch est correcte ğŸ‰")
    else:
        diff = abs(our_acc - sklearn_acc)
        print(f"\n  âš ï¸ DiffÃ©rence de {diff:.4f} â€” peut venir de la normalisation")
        
    print(f"\n  ğŸ“Š Rapport sklearn :")
    print(sklearn_report(y_test, sklearn_pred))
    
except ImportError:
    print("  âš ï¸ scikit-learn n'est pas installÃ©.")
    print("  Installe-le avec : pip install scikit-learn")
    print(f"\n  Notre KNN from scratch â†’ Accuracy = {best_acc:.4f} âœ…")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¨ SECTION 10 : FRONTIÃˆRES DE DÃ‰CISION (VISUALISATION AVANCÃ‰E)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      ğŸ¨ SECTION 10 : FRONTIÃˆRES DE DÃ‰CISION (2D)            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

On prend 2 features (PetalLength, PetalWidth) pour visualiser
comment KNN "dÃ©coupe" l'espace en zones de dÃ©cision.
""")

# Prendre uniquement les 2 features pÃ©tales (les plus discriminantes)
X_2d = df[['PetalLengthCm', 'PetalWidthCm']].values
X_2d_norm = min_max_normalize(X_2d)

X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split_manual(
    X_2d_norm, y, test_size=0.2, random_seed=42
)

# CrÃ©er une grille de points pour colorier l'arriÃ¨re-plan
h = 0.02  # rÃ©solution de la grille
x_min, x_max = X_2d_norm[:, 0].min() - 0.1, X_2d_norm[:, 0].max() + 0.1
y_min, y_max = X_2d_norm[:, 1].min() - 0.1, X_2d_norm[:, 1].max() + 0.1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# PrÃ©dire pour chaque point de la grille
grid_points = np.c_[xx.ravel(), yy.ravel()]
model_2d = KNN(k=best_k, metric='euclidean')
model_2d.fit(X_train_2d, y_train_2d)

print("â³ Calcul des frontiÃ¨res de dÃ©cision (peut prendre quelques secondes)...")
Z = model_2d.predict(grid_points)

# Convertir les labels en nombres pour le colormapping
label_to_num = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}
Z_num = np.array([label_to_num[z] for z in Z])
Z_num = Z_num.reshape(xx.shape)

# â”€â”€ Figure 5 : Decision Boundaries pour diffÃ©rentes valeurs de K â”€â”€
fig, axes = plt.subplots(2, 3, figsize=(20, 12))
k_values_plot = [1, 3, 5, 7, 11, 21]

from matplotlib.colors import ListedColormap
cmap_bg = ListedColormap(['#FFB3B3', '#B3E8E5', '#B3D9EC'])
cmap_pts = ListedColormap(['#FF6B6B', '#4ECDC4', '#45B7D1'])

for idx, k_val in enumerate(k_values_plot):
    ax = axes[idx // 3][idx % 3]
    
    model_k2d = KNN(k=k_val, metric='euclidean')
    model_k2d.fit(X_train_2d, y_train_2d)
    Z_k = model_k2d.predict(grid_points)
    Z_k_num = np.array([label_to_num[z] for z in Z_k]).reshape(xx.shape)
    
    # Fond colorÃ©
    ax.contourf(xx, yy, Z_k_num, alpha=0.3, cmap=cmap_bg)
    ax.contour(xx, yy, Z_k_num, colors='gray', linewidths=0.5, alpha=0.5)
    
    # Points d'entraÃ®nement
    for species in species_list:
        mask = y_train_2d == species
        ax.scatter(X_train_2d[mask, 0], X_train_2d[mask, 1],
                  c=colors[species], label=species, edgecolors='white',
                  s=60, alpha=0.9)
    
    acc_2d = model_k2d.score(X_test_2d, y_test_2d)
    ax.set_title(f'K = {k_val}  (Accuracy: {acc_2d:.1%})', fontsize=13, fontweight='bold')
    ax.set_xlabel('Petal Length (normalisÃ©)')
    ax.set_ylabel('Petal Width (normalisÃ©)')
    
    if idx == 0:
        ax.legend(fontsize=8, loc='upper left')

plt.suptitle('ğŸ¨ FrontiÃ¨res de DÃ©cision KNN â€” Impact du paramÃ¨tre K', 
             fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig(os.path.join(output_dir, '05_decision_boundaries.png'), dpi=150, bbox_inches='tight')
plt.close()
print("âœ… Figure 5 sauvegardÃ©e : visualisations/05_decision_boundaries.png")

print("""
ğŸ’¡ OBSERVATIONS SUR LES FRONTIÃˆRES :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  K=1  â†’ FrontiÃ¨res trÃ¨s irrÃ©guliÃ¨res (overfitting au bruit)
  K=3  â†’ Plus lisses, mais encore quelques irrÃ©gularitÃ©s
  K=5  â†’ Bon compromis âœ…
  K=11 â†’ TrÃ¨s lisses, mais risque de perdre des dÃ©tails
  K=21 â†’ Peut-Ãªtre TROP lisse (underfitting)
""")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š SECTION 11 : VISUALISATION DE LA MATRICE DE CONFUSION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ğŸ“Š SECTION 11 : MATRICE DE CONFUSION VISUELLE           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

fig, ax = plt.subplots(figsize=(8, 6))
cm_display, cm_classes = matrice_de_confusion(y_test, y_pred)

im = ax.imshow(cm_display, interpolation='nearest', cmap='Blues')
plt.colorbar(im)

short_classes = [c.replace('Iris-', '') for c in cm_classes]
ax.set_xticks(range(len(cm_classes)))
ax.set_xticklabels(short_classes, fontsize=12)
ax.set_yticks(range(len(cm_classes)))
ax.set_yticklabels(short_classes, fontsize=12)

# Ajouter les nombres dans les cellules
for i in range(len(cm_classes)):
    for j in range(len(cm_classes)):
        color = 'white' if cm_display[i, j] > cm_display.max() / 2 else 'black'
        ax.text(j, i, str(cm_display[i, j]), ha='center', va='center',
                fontsize=18, fontweight='bold', color=color)

ax.set_xlabel('Classe PrÃ©dite', fontsize=13, fontweight='bold')
ax.set_ylabel('Classe RÃ©elle', fontsize=13, fontweight='bold')
ax.set_title(f'Matrice de Confusion â€” KNN (K={best_k})\nAccuracy: {best_acc:.1%}', 
             fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig(os.path.join(output_dir, '06_confusion_matrix.png'), dpi=150, bbox_inches='tight')
plt.close()
print("âœ… Figure 6 sauvegardÃ©e : visualisations/06_confusion_matrix.png")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š SECTION 12 : IMPACT DE LA NORMALISATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      ğŸ“Š SECTION 12 : IMPACT DE LA NORMALISATION             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Sans normalisation
model_raw = KNN(k=best_k)
model_raw.fit(X_train, y_train)
acc_raw = model_raw.score(X_test, y_test)

# Avec Min-Max
model_mm = KNN(k=best_k)
model_mm.fit(X_train_norm, y_train)
acc_mm = model_mm.score(X_test_norm, y_test)

# Avec Z-Score
X_train_z = z_score_normalize(X_train)
X_test_z = z_score_normalize(X_test)
model_z = KNN(k=best_k)
model_z.fit(X_train_z, y_train)
acc_z = model_z.score(X_test_z, y_test)

print(f"  Sans normalisation   â†’ Accuracy = {acc_raw:.4f}")
print(f"  Min-Max (0 Ã  1)      â†’ Accuracy = {acc_mm:.4f}")
print(f"  Z-Score (Î¼=0, Ïƒ=1)   â†’ Accuracy = {acc_z:.4f}")

print("""
ğŸ’¡ CONCLUSION :
   Sur Iris, l'impact est faible car les features ont des Ã©chelles similaires.
   Sur d'autres datasets (ex: house-price avec surface en mÂ² et chambres en unitÃ©s),
   la normalisation est CRUCIALE !
""")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ SECTION 13 : RÃ‰CAPITULATIF ET CONCEPTS RETENUS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     ğŸ“ RÃ‰CAPITULATIF â€” KNN FROM SCRATCH                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… CE QUE TU AS APPRIS :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. KNN = "Dis-moi qui sont tes K voisins, je te dirai qui tu es"
  2. C'est un LAZY LEARNER : pas de phase d'entraÃ®nement rÃ©el
  3. La distance euclidienne est la plus utilisÃ©e
  4. K est l'hyperparamÃ¨tre clÃ© (impair, â‰¤ âˆšN)
  5. La normalisation est essentielle pour KNN
  6. Les frontiÃ¨res de dÃ©cision deviennent plus lisses quand K augmente

âœ… CE QUE TU AS CODÃ‰ FROM SCRATCH :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ 3 mÃ©triques de distance (Euclidienne, Manhattan, Minkowski)
  â€¢ La classe KNN complÃ¨te (fit, predict, predict_proba, score)
  â€¢ Train/Test Split
  â€¢ Matrice de confusion
  â€¢ Rapport de classification (PrÃ©cision, Rappel, F1)
  â€¢ Min-Max et Z-Score normalisation

âœ… CONCEPTS-CLÃ‰S POUR LINKEDIN :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ "Lazy Learner" vs "Eager Learner"
  â€¢ HyperparamÃ¨tre K et comment le choisir
  â€¢ FrontiÃ¨res de dÃ©cision
  â€¢ L'importance de la normalisation
  â€¢ Accuracy vs F1-Score

ğŸ”œ PROCHAINE Ã‰TAPE :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Semaine 2 â†’ RÃ©gression LinÃ©aire from scratch (dataset: house-price.csv)
  
  "De la classification Ã  la rÃ©gression : au lieu de prÃ©dire une CLASSE,
   on va prÃ©dire un NOMBRE (le prix d'une maison ğŸ )"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ BROUILLON DE POST LINKEDIN :

ğŸ”¥ J'ai codÃ© KNN from scratch en Python ğŸ â€” 0 librairie ML, juste NumPy !

Semaine 1 de mon parcours Machine Learning "from scratch".

L'idÃ©e de KNN est simple : pour classer un nouveau point, regarde ses K 
voisins les plus proches et vote pour la classe majoritaire.

ğŸ“š Ce que j'ai appris :
â€¢ La distance euclidienne â€” le thÃ©orÃ¨me de Pythagore appliquÃ© au ML
â€¢ L'impact du paramÃ¨tre K sur les frontiÃ¨res de dÃ©cision
â€¢ Pourquoi la normalisation est CRUCIALE pour KNN
â€¢ La diffÃ©rence entre Accuracy, PrÃ©cision et Rappel

ğŸ’» Ce que j'ai codÃ© :
â€¢ KNN complet from scratch (~250 lignes Python)
â€¢ 3 mÃ©triques de distance
â€¢ Matrice de confusion & rapport de classification
â€¢ Visualisation des frontiÃ¨res de dÃ©cision

ğŸ“Š RÃ©sultat : 96.7% de prÃ©cision sur le dataset Iris â€” identique Ã  scikit-learn !

ğŸ§  La leÃ§on : Avant d'utiliser une librairie, comprendre l'algorithme 
par soi-mÃªme change complÃ¨tement ta faÃ§on de penser la Data Science.

#MachineLearning #Python #DataScience #AI #FromScratch #KNN

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("\nğŸ Fin du cours KNN ! Toutes les visualisations sont dans le dossier 'visualisations/'")
print("   â†’ 01_exploration_iris.png")
print("   â†’ 02_distributions.png")
print("   â†’ 03_correlation_matrix.png")
print("   â†’ 04_best_k.png")
print("   â†’ 05_decision_boundaries.png")
print("   â†’ 06_confusion_matrix.png")
